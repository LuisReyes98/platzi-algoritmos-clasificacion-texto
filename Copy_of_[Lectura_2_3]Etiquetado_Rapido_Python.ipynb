{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu9C9XvfOl_J"
      },
      "source": [
        "# Etiquetado en NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AlvswJiYTdj"
      },
      "source": [
        "## Pipeline básico para Ingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmb2Cr7v92E7",
        "outputId": "c21a90df-edb1-4ef4-9a25-a96714509318"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/luis/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/luis/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencias previas\n",
        "import nltk\n",
        "nltk.download('punkt') # tokenizer tokenizador\n",
        "nltk.download('averaged_perceptron_tagger') # tagger etiquetador\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lefQX__OtfL",
        "outputId": "c5a46fa6-3b32-4c7d-98cd-421313a3173e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('And', 'CC'),\n",
              " ('now', 'RB'),\n",
              " ('here', 'RB'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('example', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('class', 'NN'),\n",
              " ('today', 'NN')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Etiquetado en una línea ...\n",
        "\n",
        "text = word_tokenize(\"And now here is the example of the class today\")\n",
        "# text = word_tokenize(\"And now here I am enjoying today\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qocvMilXPdXs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "None\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "None\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "None\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "None\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "None\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /home/luis/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Categoria gramatical de cada etiqueta\n",
        "nltk.download('tagsets')\n",
        "for tag in ['CC', 'RB', 'PRP', 'VBP', 'VBG', 'NN']:\n",
        "  print(nltk.help.upenn_tagset(tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypIhOsfKMr64",
        "outputId": "3227e404-8c1d-4949-aa51-98d9d43da087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "None\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "None\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /home/luis/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# descargamos los conjuntos de etiquetas que son\n",
        "# la metadata de que significa cada etiqueta\n",
        "nltk.download('tagsets')\n",
        "for tag in ['CC', 'RB', 'PRP']:\n",
        "  print(nltk.help.upenn_tagset(tag))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Palabras homónimas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "MEo2MWrkPqrO",
        "outputId": "3a57539a-8a0f-4476-e722-c65014e43926"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " ('not', 'RB'),\n",
              " ('permit', 'VB'),\n",
              " ('other', 'JJ'),\n",
              " ('people', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('residence', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Palabras homónimas\n",
        "text = word_tokenize(\"They do not permit other people to get a residence permit\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIIi46WzYZvc"
      },
      "source": [
        "## Etiquetado en Español \n",
        "\n",
        "Para el ingles, NLTK tiene tokenizador y etiquetador pre-entrenados por defecto. En cambio, para otros idiomas es preciso entrenarlo previamente. \n",
        "\n",
        "* usamos el corpus `cess_esp` https://mailman.uib.no/public/corpora/2007-October/005448.html\n",
        "\n",
        "* el cual usa una convención de etiquetas gramaticales dada por el grupo EAGLES https://www.cs.upc.edu/~nlp/tools/parole-sp.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "1uLX4KgDX-cs",
        "outputId": "37b51208-4c6c-4ca7-a969-c67a9ac040c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /home/luis/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('cess_esp')\n",
        "from nltk.corpus import cess_esp as cess\n",
        "from nltk import UnigramTagger as ut\n",
        "from nltk import BigramTagger as bt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiendo del tagger por unigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "mFi7BLojYw-F",
        "outputId": "35b1c62f-46fe-4f5d-f8b9-2ee0525e7083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8238/3254335735.py:7: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  uni_tagger.evaluate(cess_sents[fraction+1:]) # lo evaluamos sobre el 10% restante\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8069484240687679"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Entrenamiendo del tagger por unigramas\n",
        "cess_sents = cess.tagged_sents()\n",
        "\n",
        "fraction = int(len(cess_sents)*90/100) # 90% del dataset\n",
        "# fraction del dataset\n",
        "uni_tagger = ut(cess_sents[:fraction]) # entrenamos el etiquetador sobre el 90% del dataset\n",
        "uni_tagger.evaluate(cess_sents[fraction+1:]) # lo evaluamos sobre el 10% restante"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('A', 'sps00'),\n",
              " ('mi', 'dp1css'),\n",
              " ('me', 'pp1cs000'),\n",
              " ('gusta', 'vmip3s0'),\n",
              " ('correr', 'vmn0000'),\n",
              " ('por', 'sps00'),\n",
              " ('la', 'da0fs0'),\n",
              " ('tarde.', None)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_tagger.tag(\"A mi me gusta correr por la tarde.\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "HHA32_xVg81K",
        "outputId": "55a64d99-9465-4337-f6ee-42bafd25cb84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Yo', 'pp1csn00'),\n",
              " ('soy', 'vsip1s0'),\n",
              " ('una', 'di0fs0'),\n",
              " ('persona', 'ncfs000'),\n",
              " ('muy', 'rg'),\n",
              " ('amable', None)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento del tagger por bigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "kDIt1k1ubB9D",
        "outputId": "cffe9d86-fd47-4c4a-d57f-69702843f307"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8238/3829010840.py:4: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  bi_tagger.evaluate(cess_sents[fraction + 1:],)  # evaluamos el resultado sobre el 10% restante\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.1095272206303725"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Entrenamiento del tagger por bigramas\n",
        "fraction = int(len(cess_sents)*90/100) # 90% del dataset\n",
        "bi_tagger = bt(cess_sents[:fraction]) # entrenamos el etiquetador sobre el 90% del dataset\n",
        "bi_tagger.evaluate(cess_sents[fraction + 1:],)  # evaluamos el resultado sobre el 10% restante\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "nLCTRHeHfiY3",
        "outputId": "bfc54ac9-6a05-4a2b-d541-72382d29bd94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Yo', 'pp1csn00'),\n",
              " ('soy', 'vsip1s0'),\n",
              " ('una', None),\n",
              " ('persona', None),\n",
              " ('muy', None),\n",
              " ('amable', None)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bi_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('A', 'sps00'),\n",
              " ('mi', 'dp1css'),\n",
              " ('me', None),\n",
              " ('gusta', None),\n",
              " ('correr', None),\n",
              " ('por', None),\n",
              " ('la', None),\n",
              " ('tarde.', None)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bi_tagger.tag(\"A mi me gusta correr por la tarde.\".split())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "el etiquetado por bigramas no es muy bueno y no se recomienda usarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cess_sents = cess.tagged_sents()\n",
        "uni_tagger_100 = ut(cess_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('A', 'sps00'),\n",
              " ('mi', 'dp1css'),\n",
              " ('me', 'pp1cs000'),\n",
              " ('gusta', 'vmip3s0'),\n",
              " ('correr', 'vmn0000'),\n",
              " ('por', 'sps00'),\n",
              " ('la', 'da0fs0'),\n",
              " ('tarde.', None)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_tagger_100.tag(\"A mi me gusta correr por la tarde.\".split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Yo', 'pp1csn00'),\n",
              " ('soy', 'vsip1s0'),\n",
              " ('una', 'di0fs0'),\n",
              " ('persona', 'ncfs000'),\n",
              " ('muy', 'rg'),\n",
              " ('amable', 'aq0cs0')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_tagger_100.tag(\"Yo soy una persona muy amable\".split(\" \"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m50bRKLlikbx"
      },
      "source": [
        "# Etiquetado mejorado con Stanza (StanfordNLP)\n",
        "\n",
        "**¿Que es Stanza?**\n",
        "\n",
        "* El grupo de investigacion en NLP de Stanford tenía una suite de librerias que ejecutaban varias tareas de NLP, esta suite se unifico en un solo servicio que llamaron **CoreNLP** con base en codigo java: https://stanfordnlp.github.io/CoreNLP/index.html\n",
        "\n",
        "* Para python existe **StanfordNLP**: https://stanfordnlp.github.io/stanfordnlp/index.html\n",
        "\n",
        "* Sin embargo, **StanfordNLP** ha sido deprecado y las nuevas versiones de la suite de NLP reciben mantenimiento bajo el nombre de **Stanza**: https://stanfordnlp.github.io/stanza/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/luis/projects/cursos/platzi-curso-algoritmos-clasificacion-texto/.venv/bin/pip\n"
          ]
        }
      ],
      "source": [
        "!which pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "WY_YTM6TirMd",
        "outputId": "241e0bb1-e160-436a-904f-7bedc7ae01e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting protobuf\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting torch>=1.3.0\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from stanza) (1.16.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from stanza) (4.64.1)\n",
            "Collecting requests\n",
            "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting wheel\n",
            "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza) (59.6.0)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Using legacy 'setup.py install' for emoji, since package 'wheel' is not installed.\n",
            "Installing collected packages: charset-normalizer, wheel, urllib3, typing-extensions, protobuf, nvidia-cuda-nvrtc-cu11, numpy, idna, emoji, certifi, requests, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, stanza\n",
            "  Running setup.py install for emoji ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed certifi-2022.12.7 charset-normalizer-3.0.1 emoji-2.2.0 idna-3.4 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 protobuf-4.21.12 requests-2.28.2 stanza-1.4.2 torch-1.13.1 typing-extensions-4.4.0 urllib3-1.26.14 wheel-0.38.4\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "HG-Sdn6QmHgR",
        "outputId": "dcd469a4-37f0-4230-f24b-4335aea9a118"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 14.0MB/s]                    \n",
            "2023-02-07 10:41:46 INFO: Downloading default packages for language: es (Spanish) ...\n",
            "2023-02-07 10:41:48 INFO: File exists: /home/luis/stanza_resources/es/default.zip\n",
            "2023-02-07 10:41:52 INFO: Finished downloading models and saved to /home/luis/stanza_resources.\n"
          ]
        }
      ],
      "source": [
        "# esta parte puede demorar un poco ....\n",
        "import stanza\n",
        "stanza.download('es')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "B2sKNXawmWH7",
        "outputId": "b1cfbeea-b82c-4100-fa0c-97a9203ffbc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-07 10:51:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 17.1MB/s]                    \n",
            "2023-02-07 10:51:54 WARNING: Language es package default expects mwt, which has been added\n",
            "2023-02-07 10:51:54 INFO: Loading these models for language: es (Spanish):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ancora  |\n",
            "| mwt       | ancora  |\n",
            "| pos       | ancora  |\n",
            "=======================\n",
            "\n",
            "2023-02-07 10:51:54 INFO: Use device: gpu\n",
            "2023-02-07 10:51:54 INFO: Loading: tokenize\n",
            "2023-02-07 10:51:54 INFO: Loading: mwt\n",
            "2023-02-07 10:51:55 INFO: Loading: pos\n",
            "2023-02-07 10:51:55 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "nlp = stanza.Pipeline('es', processors='tokenize, pos')\n",
        "doc = nlp('yo soy una persona muy amable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{\n",
              "   \"id\": 1,\n",
              "   \"text\": \"yo\",\n",
              "   \"upos\": \"PRON\",\n",
              "   \"xpos\": \"pp1csn00\",\n",
              "   \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n",
              "   \"start_char\": 0,\n",
              "   \"end_char\": 2\n",
              " },\n",
              " {\n",
              "   \"id\": 2,\n",
              "   \"text\": \"soy\",\n",
              "   \"upos\": \"AUX\",\n",
              "   \"xpos\": \"vsip1s0\",\n",
              "   \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\",\n",
              "   \"start_char\": 3,\n",
              "   \"end_char\": 6\n",
              " },\n",
              " {\n",
              "   \"id\": 3,\n",
              "   \"text\": \"una\",\n",
              "   \"upos\": \"DET\",\n",
              "   \"xpos\": \"di0fs0\",\n",
              "   \"feats\": \"Definite=Ind|Gender=Fem|Number=Sing|PronType=Art\",\n",
              "   \"start_char\": 7,\n",
              "   \"end_char\": 10\n",
              " },\n",
              " {\n",
              "   \"id\": 4,\n",
              "   \"text\": \"persona\",\n",
              "   \"upos\": \"NOUN\",\n",
              "   \"xpos\": \"ncfs000\",\n",
              "   \"feats\": \"Gender=Fem|Number=Sing\",\n",
              "   \"start_char\": 11,\n",
              "   \"end_char\": 18\n",
              " },\n",
              " {\n",
              "   \"id\": 5,\n",
              "   \"text\": \"muy\",\n",
              "   \"upos\": \"ADV\",\n",
              "   \"xpos\": \"rg\",\n",
              "   \"start_char\": 19,\n",
              "   \"end_char\": 22\n",
              " },\n",
              " {\n",
              "   \"id\": 6,\n",
              "   \"text\": \"amable\",\n",
              "   \"upos\": \"ADJ\",\n",
              "   \"xpos\": \"aq0cs0\",\n",
              "   \"feats\": \"Number=Sing\",\n",
              "   \"start_char\": 23,\n",
              "   \"end_char\": 29\n",
              " }]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.sentences[0].words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "86rL3IJRm1oI",
        "outputId": "d6a637a1-1f83-4cbc-b854-1be92012ddfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yo PRON\n",
            "soy AUX\n",
            "una DET\n",
            "persona NOUN\n",
            "muy ADV\n",
            "amable ADJ\n"
          ]
        }
      ],
      "source": [
        "for sentence in doc.sentences:\n",
        "  for word in sentence.words:\n",
        "    print(word.text, word.pos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La libreria stanza es una libreria muy poderosa creada por Stanford con herramientas muy poderosas y faciles de usar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsZF93NQodxX"
      },
      "source": [
        "# Referencias adicionales:\n",
        "\n",
        "* Etiquetado POS con Stanza https://stanfordnlp.github.io/stanza/pos.html#accessing-pos-and-morphological-feature-for-word\n",
        "\n",
        "* Stanza | Github: https://github.com/stanfordnlp/stanza\n",
        "\n",
        "* Articulo en ArXiv: https://arxiv.org/pdf/2003.07082.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2gkMtDvyK_F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb1b70db904c611016fc2099f45df5cf3be837b12af04650d45f35d84bc2c94"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
